# repo-main/services/conversation_service.py

import logging
import json
from typing import Dict, List, Any, Optional
from collections import defaultdict
from openai import OpenAI
from config import Config
from services.domain.detection.detection_service import DetectionService
from clients.line_client import LineClient, COMMON_QR
from linebot.models import FlexSendMessage, QuickReply # <--- å°‡ QuickReply æ·»åŠ åˆ°é€™è£¡

logger = logging.getLogger(__name__)

class ConversationService:
    """
    è² è²¬ç®¡ç†ç”¨æˆ¶å°è©±æµç¨‹ã€ç‹€æ…‹å’Œç”Ÿæˆå›è¦†ã€‚
    å”èª¿æª¢æ¸¬æœå‹™å’Œ LINE å®¢æˆ¶ç«¯ã€‚
    """
    def __init__(self, detection_service: DetectionService, line_client: LineClient):
        self.detection_service = detection_service
        self.line_client = line_client

        # ç”¨æ–¼å„²å­˜æ¯å€‹ç”¨æˆ¶çš„ç•¶å‰æœƒè©±ç‹€æ…‹ï¼Œä¾‹å¦‚æœ€å¾Œçš„æª¢æ¸¬çµæœ
        self.STATE = defaultdict(lambda: {"risk": 0, "money_calls": 0, "last_result": {}})
        # ç”¨æ–¼å„²å­˜ç”¨æˆ¶èŠå¤©æ­·å²
        self.user_chat_history = defaultdict(list)

        self.openai_client = None
        if Config.OPENAI_API_KEY:
            try:
                self.openai_client = OpenAI(api_key=Config.OPENAI_API_KEY)
                logger.info("ConversationService: OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
            except Exception as e:
                logger.error(f"ConversationService: åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯å¤±æ•—ï¼š{e}", exc_info=True)
                self.openai_client = None
        else:
            logger.warning("ConversationService: OPENAI_API_KEY æœªè¨­å®šï¼ŒLLM ç›¸é—œåŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")


    def handle_message(self, user_id: str, message_text: str, reply_token: str):
        """
        è™•ç†æ¥æ”¶åˆ°çš„æ–‡å­—è¨Šæ¯ã€‚
        """
        logger.info(f"è™•ç†è¨Šæ¯: User ID={user_id}, Message='{message_text}'")

        # --- ç‰¹æ®ŠæŒ‡ä»¤è™•ç†ï¼šã€Œä¸‹ä¸€æ®µåµæ¸¬ã€---
        if message_text == "ä¸‹ä¸€æ®µåµæ¸¬":
            self.STATE[user_id]["last_result"] = {} # é‡ç½®ä¸Šä¸€å€‹æª¢æ¸¬çµæœ
            self.user_chat_history[user_id].clear() # æ¸…é™¤èŠå¤©æ­·å²
            logger.info(f"User {user_id} é‡ç½®åµæ¸¬ç‹€æ…‹ã€‚")
            reset_bubble_content = {
              "type":"bubble",
              "body":{"type":"box","layout":"vertical","contents":[
                {"type":"text",
                 "text":"ğŸ“© è«‹å‚³é€ä¸‹ä¸€æ®µå°è©±ï¼Œæˆ‘æœƒé‡æ–°é–‹å§‹åµæ¸¬ã€‚",
                 "wrap":True, "align":"center"}
              ]}
            }
            self.line_client.reply_flex(reply_token, self._build_flex_message_from_content(
                alt_text="é‡ç½®åµæ¸¬", contents=reset_bubble_content, quick_reply=COMMON_QR))
            return

        # --- ç‰¹æ®ŠæŒ‡ä»¤è™•ç†ï¼šã€ŒèŠèŠæ›´å¤šã€---
        if message_text == "èŠèŠæ›´å¤š":
            logger.info(f"User {user_id} è«‹æ±‚ã€èŠèŠæ›´å¤šã€ã€‚")
            history = self.user_chat_history.get(user_id, [])
            if not history:
                self.line_client.reply_text(reply_token, "ç›®å‰æ²’æœ‰èŠå¤©ç´€éŒ„å¯ä»¥å»¶ä¼¸å–”ï¼")
                return

            if not self.openai_client:
                logger.warning("OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–æˆ– API Key ç„¡æ•ˆï¼Œç„¡æ³•æä¾›ã€èŠèŠæ›´å¤šã€åŠŸèƒ½ã€‚")
                self.line_client.reply_text(reply_token, "æŠ±æ­‰ï¼ŒAI åŠŸèƒ½ç›®å‰ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æª¢æŸ¥ API Key æˆ–é…é¡ã€‚")
                return

            prompt_history = "\n".join(history[-5:]) # åªå–æœ€è¿‘çš„ 5 æ¢è¨Šæ¯
            prompt = "ä»¥ä¸‹æ˜¯æˆ‘å’Œå°æ–¹çš„å°è©±ç´€éŒ„ï¼š\n" + prompt_history + "\nè«‹åŸºæ–¼é€™äº›å…§å®¹ï¼Œç¹¼çºŒå’Œæˆ‘èŠå¤©ã€‚"

            try:
                rsp = self.openai_client.chat.completions.create(
                  model="gpt-4o-mini",
                  messages=[{"role":"user","content":prompt}]
                )
                self.line_client.reply_text(reply_token, rsp.choices[0].message.content)
            except Exception as e:
                logger.error(f"ChatGPT ã€èŠèŠæ›´å¤šã€å¤±æ•—ï¼š{e}", exc_info=True)
                self.line_client.reply_text(reply_token, "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•æä¾›æ›´å¤šå°è©±ã€‚è«‹ç¢ºèª OpenAI API Key æˆ–é…é¡æ˜¯å¦æ­£å¸¸ã€‚")
            return


        # --- ä¸»è¦è¨Šæ¯åˆ†ææµç¨‹ ---
        self.user_chat_history[user_id].append(message_text) # å„²å­˜ç•¶å‰è¨Šæ¯

        result = self.detection_service.analyze_message(message_text) # å‘¼å«æª¢æ¸¬æœå‹™
        self.STATE[user_id]["last_result"] = result # å„²å­˜æœ€è¿‘ä¸€æ¬¡çµæœ

        flex_message_to_send = self._build_detection_flex_message(result) # æ§‹å»º Flex Message
        self.line_client.reply_flex(reply_token, flex_message_to_send)


    def handle_postback(self, user_id: str, data: str, reply_token: str):
        """
        è™•ç†æ¥æ”¶åˆ°çš„ Postback äº‹ä»¶ã€‚
        """
        logger.info(f"è™•ç† Postback: User ID={user_id}, Data={data}")

        last_result = self.STATE[user_id].get("last_result", {})
        if not last_result or last_result.get("stage") is None:
            logger.warning(f"Postback received but last_result is invalid for user {user_id}. Sending prompt.")
            self.line_client.reply_text(reply_token, "æŠ±æ­‰ï¼Œè«‹æ‚¨å…ˆå‚³é€ä¸€æ®µå°è©±ï¼Œæˆ‘æ‰èƒ½ç‚ºæ‚¨åˆ†æä¸¦æä¾›åˆ¤æ–·ä¾æ“šæˆ–é˜²ç¯„å»ºè­°ã€‚")
            return

        if data == "action=explain":
            response_text = self._explain_classification(user_id)
            self.line_client.reply_text(reply_token, response_text)
        elif data == "action=prevent":
            response_text = self._prevention_suggestions(user_id)
            self.line_client.reply_text(reply_token, response_text)

    def _explain_classification(self, user_id: str) -> str:
        """
        æ ¹æ“šç”¨æˆ¶ä¸Šæ¬¡çš„è©é¨™æª¢æ¸¬çµæœï¼Œç”Ÿæˆè§£é‡‹æ–‡æœ¬ã€‚
        """
        if not self.openai_client:
            logger.warning("OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–æˆ– API Key ç„¡æ•ˆï¼Œç„¡æ³•æä¾›è§£é‡‹ã€‚")
            return "æŠ±æ­‰ï¼ŒAI åŠŸèƒ½ç›®å‰ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æª¢æŸ¥ API Key æˆ–é…é¡ã€‚"

        last = self.STATE[user_id].get("last_result")
        if not last or last.get("stage") is None:
            logger.warning(f"_explain_classification: User {user_id} last_result is invalid or missing stage.")
            return "æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ä¸Šæ¬¡çš„æª¢æ¸¬çµæœï¼Œç„¡æ³•è§£é‡‹åˆ¤æ–·ä¾æ“šã€‚è«‹å…ˆå‚³é€è¨Šæ¯è®“æˆ‘åˆ†æã€‚"

        # ç¢ºä¿ stage åœ¨ STAGE_INFO ä¸­å­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨é è¨­å€¼
        stage_num = last.get("stage", 0)
        # å¾ detection_service ç²å– stage_name
        stage_name_for_explain = self.detection_service.get_stage_info(stage_num)[0]

        # çµ„åˆè§¸ç™¼å› å­åç¨±ï¼Œå¦‚æœæ²’æœ‰å‰‡é¡¯ç¤ºã€Œç„¡ã€
        trigger_factors = "ã€".join([
            self.detection_service.get_label_desc(lab)[0] # å¾ detection_service ç²å–æ¨™ç±¤æè¿°
            for lab in last.get("labels", [])
        ]) or "ç„¡"

        prompt = (
          f"æˆ‘å‰›å‰›åµæ¸¬åˆ°ä¸€å€‹è¨Šæ¯ï¼Œåˆ†é¡çµæœç‚ºéšæ®µ {stage_num}ï¼ˆ{stage_name_for_explain}ï¼‰ï¼Œ"
          f"è§¸ç™¼å› å­æœ‰ {trigger_factors}ã€‚"
          "è«‹ç”¨ 2ï½3 å¥è©±ç°¡å–®èªªæ˜ç‚ºä½•æœƒåšå‡ºé€™æ¨£çš„åˆ¤æ–·ã€‚"
        )
        try:
            rsp = self.openai_client.chat.completions.create(
              model="gpt-4o-mini",
              messages=[{"role":"user", "content":prompt}]
            )
            return rsp.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"è§£é‡‹åˆ¤æ–·å¤±æ•—ï¼š{e}", exc_info=True)
            return "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•æä¾›åˆ¤æ–·èªªæ˜ã€‚è«‹ç¢ºèª OpenAI API Key æˆ–é…é¡æ˜¯å¦æ­£å¸¸ã€‚"


    def _prevention_suggestions(self, user_id: str) -> str:
        """
        æ ¹æ“šç”¨æˆ¶ä¸Šæ¬¡çš„è©é¨™æª¢æ¸¬çµæœï¼Œç”Ÿæˆé˜²ç¯„å»ºè­°æ–‡æœ¬ã€‚
        """
        if not self.openai_client:
            logger.warning("OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–æˆ– API Key ç„¡æ•ˆï¼Œç„¡æ³•æä¾›é˜²ç¯„å»ºè­°ã€‚")
            return "æŠ±æ­‰ï¼ŒAI åŠŸèƒ½ç›®å‰ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æª¢æŸ¥ API Key æˆ–é…é¡ã€‚"

        last = self.STATE[user_id].get("last_result")
        if not last or last.get("stage") is None:
            logger.warning(f"_prevention_suggestions: User {user_id} last_result is invalid or missing stage.")
            return "æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ä¸Šæ¬¡çš„æª¢æ¸¬çµæœï¼Œç„¡æ³•æä¾›é˜²ç¯„å»ºè­°ã€‚è«‹å…ˆå‚³é€è¨Šæ¯è®“æˆ‘åˆ†æã€‚"

        # ç¢ºä¿ stage åœ¨ STAGE_INFO ä¸­å­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨é è¨­å€¼
        stage_num = last.get("stage", 0)
        # å¾ detection_service ç²å– stage_name
        stage_name_for_prevent = self.detection_service.get_stage_info(stage_num)[0]

        # çµ„åˆè§¸ç™¼å› å­åç¨±ï¼Œå¦‚æœæ²’æœ‰å‰‡é¡¯ç¤ºã€Œç„¡ã€
        trigger_factors = "ã€".join([
            self.detection_service.get_label_desc(lab)[0] # å¾ detection_service ç²å–æ¨™ç±¤æè¿°
            for lab in last.get("labels", [])
        ]) or "ç„¡"

        prompt = (
          f"æ ¹æ“šè©é¨™éšæ®µ {stage_num}ï¼ˆ{stage_name_for_prevent}ï¼‰ï¼Œ"
          f"è§¸ç™¼å› å­ {trigger_factors}ï¼Œ"
          "è«‹åˆ—å‡º 3 æ¢æœ€å¯¦ç”¨çš„é˜²ç¯„å»ºè­°ã€‚"
        )
        try:
            rsp = self.openai_client.chat.completions.create(
              model="gpt-4o-mini",
              messages=[{"role":"user", "content":prompt}]
            )
            return rsp.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"æä¾›é˜²ç¯„å»ºè­°å¤±æ•—ï¼š{e}", exc_info=True)
            return "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•æä¾›é˜²ç¯„å»ºè­°ã€‚è«‹ç¢ºèª OpenAI API Key æˆ–é…é¡æ˜¯å¦æ­£å¸¸ã€‚"

    def _build_flex_message_from_content(self, alt_text: str, contents: dict, quick_reply: Optional[QuickReply] = None) -> FlexSendMessage:
        """
        è¼”åŠ©å‡½æ•¸ï¼šå¾å…§å®¹å­—å…¸æ§‹å»º FlexSendMessageã€‚
        """
        return FlexSendMessage(alt_text=alt_text, contents=contents, quick_reply=quick_reply)

    # repo-main/services/conversation_service.py

    # ... å…¶ä»–ç¨‹å¼ç¢¼ ...

    def _build_detection_flex_message(self, result: dict) -> FlexSendMessage:
        """
        æ ¹æ“šè©é¨™åµæ¸¬çµæœï¼Œæ§‹å»ºä¸¦è¿”å›ä¸€å€‹ LINE Flex Message ç‰©ä»¶ã€‚
        """
        stage_num = result.get("stage", 0)
        s_name, advice = self.detection_service.get_stage_info(stage_num)

        labels = result.get("labels", [])
        reasons = "ã€".join(
            f"{self.detection_service.get_label_desc(lab)[0]}"
            for lab in labels if self.detection_service.get_label_desc(lab)
        ) or "ç„¡é¢¨éšªæ¨™ç±¤"

        # æª¢æŸ¥æ˜¯å¦æœ‰ LLM éŒ¯èª¤
        if result.get("llm_error"):
            # å¦‚æœ LLM å¤±æ•—ï¼Œä¿®æ”¹å»ºè­°è¡Œå‹•å’Œè§¸ç™¼å› å­é¡¯ç¤º
            reasons = "LLM åˆ†æå¤±æ•—"
            advice = f"AI åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ã€‚åŸå› ï¼š{result.get('error_message', 'æœªçŸ¥éŒ¯èª¤')}ã€‚è«‹æª¢æŸ¥ OpenAI é…é¡æˆ–ç¨å¾Œé‡è©¦ã€‚"
            # ä¹Ÿå¯ä»¥è€ƒæ…®æ”¹è®Šé¡è‰²æˆ–æ·»åŠ åœ–æ¨™ä¾†è¡¨ç¤ºéŒ¯èª¤
            stage_display = "âŒ åˆ†æç•°å¸¸"
            color = "#FF0000"  # ç´…è‰²è¡¨ç¤ºéŒ¯èª¤
        else:
            stage_display = f"ğŸ” ç›®å‰éšæ®µï¼š{stage_num}ï¼ˆ{s_name}ï¼‰"
            color = "#1DB446" if stage_num <= 1 else "#FF0000" if stage_num >= 3 else "#FFBB00"  # æ ¹æ“šéšæ®µæ›´æ”¹é¡è‰²

        bubble_content = {
            "type": "bubble",
            "body": {
                "type": "box", "layout": "vertical", "contents": [
                    {"type": "text", "text": stage_display, "weight": "bold", "size": "lg", "color": color},
                    {"type": "separator", "margin": "md"},
                    {"type": "text", "text": f"ğŸ“Œ è§¸ç™¼å› å­ï¼š{reasons}", "wrap": True, "margin": "md"},
                    {"type": "separator", "margin": "md"},
                    {"type": "text", "text": f"ğŸ‘‰ å»ºè­°è¡Œå‹•ï¼š{advice}", "wrap": True, "margin": "md"}
                ]
            },
            "footer": {
                "type": "box", "layout": "horizontal", "contents": [
                    {"type": "button", "style": "link", "height": "sm",
                     "action": {"type": "postback", "label": "ç‚ºä½•é€™æ¨£åˆ¤æ–·ï¼Ÿ", "data": "action=explain"}},
                    {"type": "button", "style": "link", "height": "sm",
                     "action": {"type": "postback", "label": "å¦‚ä½•é˜²ç¯„ï¼Ÿ", "data": "action=prevent"}}
                ]
            }
        }

        return self._build_flex_message_from_content(
            alt_text="è©é¨™åµæ¸¬çµæœ", contents=bubble_content, quick_reply=COMMON_QR)